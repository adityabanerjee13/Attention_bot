{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PyPDF2 import PdfReader\n",
    "import fitz\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Natural Language Toolkit (tokeniser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download nltk resources if needed\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text and create chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file and filter unwanted text.\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        state = False\n",
    "        # Extract text from each page\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            \n",
    "            # Extract text blocks\n",
    "            text_blocks = page.get_text(\"blocks\")\n",
    "            for block in text_blocks:\n",
    "                # Check if block is text (not image)\n",
    "                if block[6] == 0:  # 0 indicates text block\n",
    "                    para = block[4].replace('\\n', '')\n",
    "\n",
    "                    # Filter out unwanted text\n",
    "                    temp0 = re.search('references', para.lower())\n",
    "                    temp1 = re.search('conclusion', para.lower())\n",
    "                    temp2 = re.search('acknowledgments', para.lower())\n",
    "                    if 'figure' in (para[:15]).lower():\n",
    "                        continue\n",
    "                    elif 'table' in para[:15].lower():\n",
    "                        continue\n",
    "                    elif temp0 and temp0.start() < 10:\n",
    "                        state = True\n",
    "                        break\n",
    "                    elif temp1 and temp1.start() < 10:\n",
    "                        state = True\n",
    "                        break\n",
    "                    elif temp2 and temp2.start() < 10:\n",
    "                        state = True\n",
    "                        break\n",
    "                    # elif len(para) < 30:\n",
    "                    #     continue\n",
    "                    text += para + \"\\n\"\n",
    "            if state:\n",
    "                break\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def semantic_chunking(text, model, max_chunk_size=1000, min_chunk_size=200, similarity_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Chunk text using semantic similarity between sentences to form coherent paragraphs.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to chunk\n",
    "        model: SentenceTransformer model for encoding sentences\n",
    "        max_chunk_size: Maximum size of a chunk in characters\n",
    "        min_chunk_size: Minimum size of a chunk in characters\n",
    "        similarity_threshold: Threshold for semantic similarity to group sentences\n",
    "        \n",
    "    Returns:\n",
    "        List of semantically coherent text chunks\n",
    "    \"\"\"\n",
    "    # Split into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    if len(sentences) <= 1:\n",
    "        return [text] if text.strip() else []\n",
    "        \n",
    "    # Get embeddings for each sentence\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    \n",
    "    # Normalize embeddings for cosine similarity\n",
    "    sentence_embeddings = sentence_embeddings / np.linalg.norm(sentence_embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = [sentences[0]]\n",
    "    current_chunk_size = len(sentences[0])\n",
    "    current_avg_embedding = sentence_embeddings[0].reshape(1, -1)\n",
    "    \n",
    "    for i in range(1, len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        sentence_size = len(sentence)\n",
    "        \n",
    "        # Check semantic similarity with current chunk\n",
    "        sentence_embedding = sentence_embeddings[i].reshape(1, -1)\n",
    "        similarity = cosine_similarity(current_avg_embedding, sentence_embedding)[0][0]\n",
    "        \n",
    "        # Decision to add to current chunk or start a new one\n",
    "        add_to_current = True\n",
    "        \n",
    "        # If adding this would exceed max size, start a new chunk\n",
    "        if current_chunk_size + sentence_size > max_chunk_size:\n",
    "            add_to_current = False\n",
    "        # If similarity is below threshold and we have a decent chunk size already, start a new one\n",
    "        elif similarity < similarity_threshold and current_chunk_size >= min_chunk_size:\n",
    "            add_to_current = False\n",
    "            \n",
    "        if add_to_current:\n",
    "            # Add sentence to current chunk\n",
    "            current_chunk.append(sentence)\n",
    "            current_chunk_size += sentence_size\n",
    "            \n",
    "            # Update average embedding\n",
    "            n = len(current_chunk)\n",
    "            current_avg_embedding = ((n-1) * current_avg_embedding + sentence_embedding) / n\n",
    "        else:\n",
    "            # Complete current chunk and start a new one\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_chunk_size = sentence_size\n",
    "            current_avg_embedding = sentence_embedding\n",
    "    \n",
    "    # Add the final chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_pdfs_semantic(data_dir, model, max_chunk_size=1000, min_chunk_size=200, similarity_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Process all PDFs in the directory, creating semantic chunks, and return chunks with metadata.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing PDF files\n",
    "        model: SentenceTransformer model\n",
    "        max_chunk_size: Maximum size of a chunk in characters\n",
    "        min_chunk_size: Minimum size of a chunk in characters\n",
    "        similarity_threshold: Threshold for semantic similarity to group sentences\n",
    "        \n",
    "    Returns:\n",
    "        chunks: List of text chunks\n",
    "        chunk_metadata: List of dictionaries with metadata for each chunk\n",
    "    \"\"\"\n",
    "    pdf_files = [f for f in os.listdir(data_dir) if f.lower().endswith('.pdf')]\n",
    "    chunks = []\n",
    "    chunk_metadata = []\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files in {data_dir}\")\n",
    "    \n",
    "    for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        pdf_path = os.path.join(data_dir, pdf_file)\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "        if not text.strip():\n",
    "            continue\n",
    "            \n",
    "        # Create semantic chunks\n",
    "        document_chunks = semantic_chunking(\n",
    "            text, \n",
    "            model, \n",
    "            max_chunk_size=max_chunk_size, \n",
    "            min_chunk_size=min_chunk_size,\n",
    "            similarity_threshold=similarity_threshold\n",
    "        )\n",
    "        \n",
    "        # Add chunks and their metadata\n",
    "        for i, chunk in enumerate(document_chunks):\n",
    "            if chunk.strip():\n",
    "                chunks.append(chunk)\n",
    "                chunk_metadata.append({\n",
    "                    \"file_path\": pdf_path,\n",
    "                    \"file_name\": pdf_file,\n",
    "                    \"chunk_index\": i,\n",
    "                    \"total_chunks\": len(document_chunks),\n",
    "                    \"chunk_length\": len(chunk)\n",
    "                })\n",
    "    \n",
    "    print(f\"Created {len(chunks)} semantic chunks from {len(pdf_files)} PDF files\")\n",
    "    return chunks, chunk_metadata\n",
    "\n",
    "data_dir = \"data\"  # Directory containing PDF files\n",
    "output_dir = \"vector_db\"  # Directory to save the vector database\n",
    "\n",
    "# Chunk parameters\n",
    "max_chunk_size = 5000  # Maximum characters per chunk\n",
    "min_chunk_size = 800   # Minimum characters per chunk\n",
    "similarity_threshold = 0.7  # Similarity threshold for chunking\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load model once to use for both chunking and embedding\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "print(f\"Loading SentenceTransformer model: {model_name}\")\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Process PDFs with semantic chunking\n",
    "chunks, chunk_metadata = process_pdfs_semantic(\n",
    "    data_dir, \n",
    "    model, \n",
    "    max_chunk_size=max_chunk_size, \n",
    "    min_chunk_size=min_chunk_size,\n",
    "    similarity_threshold=similarity_threshold\n",
    ")\n",
    "\n",
    "if not chunks:\n",
    "    raise ValueError('No valid text chunks found.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Faiss vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_faiss_index(chunks, model):\n",
    "    \"\"\"Create a FAISS index from text chunks.\"\"\"\n",
    "    # Create embeddings\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "    \n",
    "    # Normalize embeddings for cosine similarity\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    \n",
    "    # Create the FAISS index\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity with normalized vectors\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index\n",
    "\n",
    "print(f\"Creating vector database from {len(chunks)} semantic chunks...\")\n",
    "\n",
    "# Create FAISS index\n",
    "index = create_faiss_index(chunks, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Vector Database and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the index and metadata\n",
    "index_path = os.path.join(output_dir, \"faiss_index.bin\")\n",
    "faiss.write_index(index, index_path)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    \"chunk_metadata\": chunk_metadata,\n",
    "    \"model_name\": model.get_sentence_embedding_dimension(),\n",
    "    \"max_chunk_size\": max_chunk_size,\n",
    "    \"min_chunk_size\": min_chunk_size,\n",
    "    \"similarity_threshold\": similarity_threshold,\n",
    "    \"chunks\": chunks  # Store the actual text chunks for retrieval\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, \"metadata.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(f\"Vector database created and saved to {output_dir}\")\n",
    "print(f\"Number of semantic chunks indexed: {len(chunks)}\")\n",
    "print(f\"Vector dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"Chunk parameters: max_size={max_chunk_size}, min_size={min_chunk_size}, similarity_threshold={similarity_threshold}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
