{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/aditya-venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-30 11:07:50.515352: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-30 11:07:50.527013: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743332870.540375  313808 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743332870.544501  313808 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743332870.554954  313808 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743332870.554971  313808 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743332870.554973  313808 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743332870.554974  313808 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-30 11:07:50.559583: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from groq import Groq\n",
    "import re\n",
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load GROQ and Phoenix API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded API Keys: {'GROQ_AI_key': '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', 'GROQ_AI_key2': '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', 'OPENAI_API_KEY': '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', 'PHOENIX_API_KEY': '48cc86219b7060d16b5:8594d2d', 'PHOENIX_API_KEY2': 'd129dcb352f09374e46:cc7e32a'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "API_KEYS_FILE = \"api_keys.json\"\n",
    "\n",
    "def load_api_keys():\n",
    "    \"\"\"Load API keys from a JSON file.\"\"\"\n",
    "    if not os.path.exists(API_KEYS_FILE):\n",
    "        print(\"API keys file not found.\")\n",
    "        return None\n",
    "    \n",
    "    with open(API_KEYS_FILE, \"r\") as f:\n",
    "        keys = json.load(f)\n",
    "    return keys\n",
    "\n",
    "keys = load_api_keys()\n",
    "print(\"Loaded API Keys:\", keys)\n",
    "\n",
    "GROQ_AI_key = keys.get(\"GROQ_AI_key2\")\n",
    "PHOENIX_API_KEY = keys.get(\"PHOENIX_API_KEY2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register and enable tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: chatbot\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: https://app.phoenix.arize.com/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PHOENIX_CLIENT_HEADERS='api_key={key}'.format(key=PHOENIX_API_KEY)\n",
    "PHOENIX_COLLECTOR_ENDPOINT='https://app.phoenix.arize.com'\n",
    "\n",
    "from phoenix.otel import register\n",
    "\n",
    "tracer_provider = register(\n",
    "  project_name=\"chatbot\",\n",
    "  endpoint=\"https://app.phoenix.arize.com/v1/traces\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tracer = tracer_provider.get_tracer(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GROQ_API_KEY'] = GROQ_AI_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHATBOT using RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def load_vector_db(vector_db_path=\"vector_db\"):\n",
    "    \"\"\"\n",
    "    Load the FAISS index and metadata from the vector database.\n",
    "    \n",
    "    Args:\n",
    "        vector_db_path: Path to the directory containing the vector database\n",
    "        \n",
    "    Returns:\n",
    "        index: FAISS index\n",
    "        metadata: Dictionary containing metadata and chunks\n",
    "        model: SentenceTransformer model used for encoding\n",
    "    \"\"\"\n",
    "    # Load FAISS index\n",
    "    index_path = os.path.join(vector_db_path, \"faiss_index.bin\")\n",
    "    index = faiss.read_index(index_path)\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(os.path.join(vector_db_path, \"metadata.pkl\"), \"rb\") as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    # Load the same model that was used for creating the embeddings\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    return {\n",
    "        \"index\": index,\n",
    "        \"metadata\": metadata,\n",
    "        \"model\": model\n",
    "    }\n",
    "\n",
    "def search_vector_db(query, vector_db_info, top_k=5, rerank=True):\n",
    "    \"\"\"\n",
    "    Search the vector database for the most similar chunks to the query.\n",
    "    \n",
    "    Args:\n",
    "        query: Query text\n",
    "        top_k: Number of top results to return\n",
    "        vector_db_path: Path to the vector database directory\n",
    "        rerank: Whether to rerank results using more sophisticated similarity\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing chunk info and similarity scores\n",
    "    \"\"\"\n",
    "    # Load the vector database\n",
    "    index = vector_db_info['index']\n",
    "    metadata = vector_db_info['metadata']\n",
    "    model = vector_db_info['model']\n",
    "    \n",
    "    # Get chunks and their metadata\n",
    "    chunks = metadata[\"chunks\"]\n",
    "    chunk_metadata = metadata[\"chunk_metadata\"]\n",
    "    \n",
    "    # Encode the query\n",
    "    query_embedding = model.encode([query])\n",
    "    \n",
    "    # Normalize for cosine similarity\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Search the index - retrieve more results than needed for reranking\n",
    "    k_search = top_k * 3 if rerank else top_k\n",
    "    k_search = min(k_search, len(chunks))  # Don't try to get more results than we have chunks\n",
    "    \n",
    "    distances, indices = index.search(query_embedding, k_search)\n",
    "    \n",
    "    # Process results\n",
    "    initial_results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx != -1:  # Valid index\n",
    "            chunk_text = chunks[idx]\n",
    "            chunk_info = chunk_metadata[idx]\n",
    "            similarity = float(distances[0][i])\n",
    "            \n",
    "            initial_results.append({\n",
    "                \"chunk_text\": chunk_text,\n",
    "                \"file_path\": chunk_info[\"file_path\"],\n",
    "                \"file_name\": chunk_info[\"file_name\"],\n",
    "                \"chunk_index\": chunk_info[\"chunk_index\"],\n",
    "                \"similarity\": similarity\n",
    "            })\n",
    "    \n",
    "    # Rerank if requested (uses more sophisticated similarity calculation)\n",
    "    if rerank and initial_results:\n",
    "        # Get more detailed embeddings for reranking\n",
    "        query_embedding_full = model.encode(query, convert_to_numpy=True)\n",
    "        chunk_texts = [result[\"chunk_text\"] for result in initial_results]\n",
    "        chunk_embeddings = model.encode(chunk_texts, convert_to_numpy=True)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity([query_embedding_full], chunk_embeddings)[0]\n",
    "        \n",
    "        # Update similarities and sort\n",
    "        for i, result in enumerate(initial_results):\n",
    "            result[\"similarity\"] = float(similarities[i])\n",
    "        \n",
    "        # Sort by new similarity scores\n",
    "        initial_results.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "        \n",
    "        # Take top_k\n",
    "        results = initial_results[:top_k]\n",
    "    else:\n",
    "        results = initial_results[:top_k]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_surrounding_context(result, metadata, context_chunks=1):\n",
    "    \"\"\"\n",
    "    Get surrounding chunks as context for a result.\n",
    "    \n",
    "    Args:\n",
    "        result: A single result dictionary from search_vector_db\n",
    "        metadata: Metadata from the vector database\n",
    "        context_chunks: Number of chunks to include before and after\n",
    "        \n",
    "    Returns:\n",
    "        String with the context including the result chunk\n",
    "    \"\"\"\n",
    "    chunks = metadata[\"chunks\"]\n",
    "    chunk_metadata = metadata[\"chunk_metadata\"]\n",
    "    \n",
    "    file_path = result[\"file_path\"]\n",
    "    chunk_index = result[\"chunk_index\"]\n",
    "    \n",
    "    # Find chunks from the same file\n",
    "    same_file_chunks = []\n",
    "    chunk_indices = []\n",
    "    \n",
    "    for i, info in enumerate(chunk_metadata):\n",
    "        if info[\"file_path\"] == file_path:\n",
    "            same_file_chunks.append(chunks[i])\n",
    "            chunk_indices.append(info[\"chunk_index\"])\n",
    "    \n",
    "    # Sort by chunk index\n",
    "    sorted_chunks = [x for _, x in sorted(zip(chunk_indices, same_file_chunks))]\n",
    "    \n",
    "    # Find the index of our chunk in the sorted list\n",
    "    file_chunk_index = chunk_indices.index(chunk_index)\n",
    "    \n",
    "    # Get context chunks\n",
    "    start_idx = max(0, file_chunk_index - context_chunks)\n",
    "    end_idx = min(len(sorted_chunks), file_chunk_index + context_chunks + 1)\n",
    "    \n",
    "    context_text = \"\\n\\n\".join(sorted_chunks[start_idx:end_idx])\n",
    "    return context_text\n",
    "\n",
    "@tracer.chain\n",
    "def retrive(query, vector_db_info, top_k=3):\n",
    "    \"\"\"\n",
    "    Perform retrival for specific the query for \"What is attention?\" and format the results.\n",
    "    \n",
    "    Args:\n",
    "        query: Query text (default: \"What is attention?\")\n",
    "        top_k: Number of top results to return\n",
    "        vector_db_path: Path to the vector database directory\n",
    "        \n",
    "    Returns:\n",
    "        None, prints formatted results\n",
    "    \"\"\"\n",
    "    # print(f\"Searching for: '{query}'\")\n",
    "    \n",
    "    # Get search results\n",
    "    results = search_vector_db(query, vector_db_info, top_k=top_k)\n",
    "    \n",
    "    # Load metadata for context\n",
    "    metadata = vector_db_info['metadata']\n",
    "    \n",
    "    # # Print results\n",
    "    # print(f\"\\nTop {len(results)} results:\")\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        # Get context with surrounding chunks\n",
    "        results[i]['chunk_text'] = get_surrounding_context(result, metadata, context_chunks=0)\n",
    "    return results\n",
    "\n",
    "def date_time_stamp(format = \"%m%d%H%M%S\"):\n",
    "    return datetime.now().strftime(format)\n",
    "\n",
    "\n",
    "def remove_think_section(text):\n",
    "    return re.sub(r\"<think>.*?</think>\\s*\", \"\", text, flags=re.DOTALL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG_chatbot:\n",
    "    def __init__(self, api_key: str = None):\n",
    "        self.client = Groq(api_key=api_key)    \n",
    "        self.history = ''\n",
    "        self.doc_gen_model = 'gemma2-9b-it'\n",
    "        self.reply_model = 'deepseek-r1-distill-qwen-32b'\n",
    "\n",
    "        self.vector_db_info = load_vector_db(vector_db_path=\"vector_db\")\n",
    "\n",
    "        with open(\"promt_engi.txt\", \"r\") as file:\n",
    "            self.chat_prompt = file.read()\n",
    "        \n",
    "    def __query(self, query: str):\n",
    "\n",
    "        # RETRIVAL PIPE-LINE\n",
    "\n",
    "        # generate hypothetical document\n",
    "        message=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Please write a brief scientific paper passage to answer the question\\nQuestion: {question}\\nPassage:\"\"\".format(question=query)\n",
    "            }\n",
    "        ]\n",
    "        generate_doc = self.client.chat.completions.create(\n",
    "            messages=message,\n",
    "            model = self.doc_gen_model,\n",
    "            temperature=0.2,\n",
    "        ).choices[0].message.content\n",
    "\n",
    "        # retrive the most relevant document using generated document\n",
    "        result = retrive(remove_think_section(generate_doc), self.vector_db_info, top_k=3)[0]['chunk_text']\n",
    "\n",
    "        # generate response using the retrieved document\n",
    "        message = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": self.chat_prompt+\"\"\"{history}\"\"\".format(history=self.history)+\"\"\"\\nUSER: {input}\"\"\".format(input=query)+\"\"\"\\nRAG output: {context}\"\"\".format(context=result)+\"\"\"\\nASSISTANT: \"\"\"\n",
    "            }\n",
    "        ]\n",
    "        response = self.client.chat.completions.create(\n",
    "            messages=message, \n",
    "            model=self.reply_model,\n",
    "            temperature=0.2,\n",
    "        ).choices[0].message.content\n",
    "        response = remove_think_section(response)\n",
    "        self.history += '\\n' + 'User: ' + query + '\\n' + 'Assistant: ' + response\n",
    "        print('\\n' + 'User: ' + query + '\\n' + 'Assistant: ' + response)\n",
    "    \n",
    "\n",
    "    def Start_conversation(self, save_trace: bool = False):\n",
    "        \"\"\"\n",
    "        Start a conversation with the chatbot.\n",
    "        Args:\n",
    "            save_trace: Whether to save the trace data\n",
    "        \"\"\"\n",
    "        with open(\"chat_greet.txt\", \"r\") as file:\n",
    "            print(file.read())\n",
    "        count = 0\n",
    "        while True:\n",
    "            count+=1\n",
    "            sys.stdout.flush()\n",
    "            query = input(\"You: \")\n",
    "            if not query or query.lower() == \"exit\" or query.lower() == \"quit\":\n",
    "                break\n",
    "            data = self.__query(query)\n",
    "        print(\"Thank you for your time and Goodbye :)\")\n",
    "\n",
    "bot = RAG_chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the AI Chatbot!\n",
      "\n",
      "Hello! I am your AI-powered chatbot, here to assist you with your questions regarding transformers and Attention is all you need. \n",
      "The challenge here is to restrict questions only to the knowledge to Attention is all you need by Vaswani et all!\n",
      "\n",
      "Type \"exit\" or \"quit\" to end the conversation.\n",
      "\n",
      "Let's get started! Ask me why attention is all you need!?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception while exporting Span.\n",
      "urllib3.exceptions.SSLError: EOF occurred in violation of protocol (_ssl.c:2417)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/aditya-venv/lib/python3.12/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/aditya-venv/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/aditya-venv/lib/python3.12/site-packages/urllib3/util/retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='app.phoenix.arize.com', port=443): Max retries exceeded with url: /v1/traces (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:2417)')))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/aditya-venv/lib/python3.12/site-packages/opentelemetry/sdk/trace/export/__init__.py\", line 113, in on_end\n",
      "    self.span_exporter.export((span,))\n",
      "  File \"/opt/anaconda3/envs/aditya-venv/lib/python3.12/site-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py\", line 189, in export\n",
      "    return self._export_serialized_spans(serialized_data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/aditya-venv/lib/python3.12/site-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py\", line 159, in _export_serialized_spans\n",
      "    resp = self._export(serialized_data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/aditya-venv/lib/python3.12/site-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py\", line 133, in _export\n",
      "    return self._session.post(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/aditya-venv/lib/python3.12/site-packages/requests/sessions.py\", line 637, in post\n",
      "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/aditya-venv/lib/python3.12/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/aditya-venv/lib/python3.12/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/aditya-venv/lib/python3.12/site-packages/requests/adapters.py\", line 698, in send\n",
      "    raise SSLError(e, request=request)\n",
      "requests.exceptions.SSLError: HTTPSConnectionPool(host='app.phoenix.arize.com', port=443): Max retries exceeded with url: /v1/traces (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:2417)')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: hi\n",
      "Assistant: Hello! I'm here to help you explore attention mechanisms and Transformer architectures. Feel free to ask any questions you have about self-attention, multi-head attention, Transformer variants, or optimizations. Let's dive in!\n",
      "Thank you for your time and Goodbye :)\n"
     ]
    }
   ],
   "source": [
    "bot.Start_conversation(save_trace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the AI Chatbot!\n",
      "\n",
      "Hello! I am your AI-powered chatbot, here to assist you with your questions regarding transformers and Attention is all you need. \n",
      "The challenge here is to restrict questions only to the knowledge to Attention is all you need by Vaswani et all!\n",
      "\n",
      "Type \"exit\" or \"quit\" to end the conversation.\n",
      "\n",
      "Let's get started! Ask me why attention is all you need!?\n",
      "\n",
      "User: hi\n",
      "Assistant: Hello! I'm here to help you with any questions related to attention mechanisms and Transformer architectures. Feel free to ask anything you'd like to know!\n",
      "\n",
      "User: What is Attention?\n",
      "Assistant: Attention mechanisms in machine learning enable models to focus on specific parts of input data, enhancing their ability to process information effectively. Introduced prominently in the Transformer model by Vaswani et al. (2017), attention allows models to weigh different input parts differently, unlike traditional RNNs or CNNs.\n",
      "\n",
      "At its core, attention involves three key components: queries, keys, and values. Queries determine what the model is looking for, keys define where the search occurs, and values are the retrieved information. The model calculates attention weights based on the similarity between queries and keys, often using a softmax function for normalization.\n",
      "\n",
      "Self-attention, a variant used in Transformers, enables the model to attend to different positions within the input sequence, capturing long-range dependencies effectively. Multi-head attention extends this by allowing the model to capture various types of relationships simultaneously.\n",
      "\n",
      "Beyond Transformers, attention mechanisms are applied in architectures like convolutional networks (e.g., CBAM) to enhance feature representation. This adaptability makes attention a versatile tool, improving model performance and interpretability by highlighting relevant input parts.\n",
      "\n",
      "In summary, attention mechanisms, particularly within Transformer models, revolutionize how models process and understand data, offering significant advantages in handling complex patterns and dependencies.\n",
      "\n",
      "User: How is transformer different from RNN or LSTM\n",
      "Assistant: Transformers differ fundamentally from RNNs and LSTMs in several key aspects:\n",
      "\n",
      "1. **Parallel Processing**: Transformers utilize self-attention mechanisms, enabling them to process the entire input sequence simultaneously. This allows for efficient parallel processing, making training faster, especially on hardware like GPUs or TPUs.\n",
      "\n",
      "2. **Handling Long-Range Dependencies**: Unlike RNNs and LSTMs, which struggle with very long sequences due to issues like vanishing gradients, Transformers can effectively capture relationships between distant parts of the sequence. This makes them particularly suited for tasks requiring understanding of long-range context.\n",
      "\n",
      "3. **Attention Mechanisms**: Transformers employ attention mechanisms that allow them to focus on relevant parts of the input dynamically. This capability enables the model to learn various types of relationships simultaneously, enhancing flexibility and power in feature extraction.\n",
      "\n",
      "In essence, Transformers offer a more efficient, scalable, and versatile approach to sequence modeling compared to traditional RNNs and LSTMs.\n",
      "\n",
      "User: HOw can we make transformers faster\n",
      "Assistant: To make transformers faster, several optimization techniques can be employed, each addressing different aspects of the model's computational complexity:\n",
      "\n",
      "1. **Efficient Attention Mechanisms**: \n",
      "   - **Sparse Attention**: Reduces the number of attention operations by focusing only on relevant parts of the sequence.\n",
      "   - **Local Attention**: Limits attention to nearby tokens, reducing the computational load.\n",
      "\n",
      "2. **Parameter Reduction Techniques**:\n",
      "   - **Weight Pruning**: Removes unnecessary weights to reduce computation.\n",
      "   - **Weight Factorization**: Decomposes weights into smaller matrices, lowering the number of operations.\n",
      "\n",
      "3. **Quantization**:\n",
      "   - Reduces the precision of weights and activations, decreasing memory usage and computation time.\n",
      "\n",
      "4. **Knowledge Distillation**:\n",
      "   - Trains a smaller model (student) to mimic a larger one (teacher), optimizing for speed without significant loss in accuracy.\n",
      "\n",
      "5. **Alternative Architectures**:\n",
      "   - **Reformer**: Uses locality-sensitive hashing and chunked attention to reduce complexity.\n",
      "   - **Sparse Transformers**: Implements sparse attention patterns to decrease computation.\n",
      "\n",
      "6. **Training and Inference Optimizations**:\n",
      "   - **Efficient Pretraining Objectives**: Such as replaced token detection, can reduce the number of tokens needed during training.\n",
      "   - **Product-Key Attention**: Enhances attention capacity with minimal computational overhead.\n",
      "\n",
      "While these methods help, they don't eliminate the quadratic scaling issue. For very long sequences, considering alternative architectures like Reformer or Sparse Transformers is recommended. Combining these techniques can lead to significant speed improvements, balancing computational efficiency with model performance.\n",
      "\n",
      "User: can you explain the 3rd point of the second to last question i asked yo\n",
      "Assistant: Certainly! The third point in the second-to-last question you asked was about **quantization**. Here's an explanation:\n",
      "\n",
      "### Quantization in Transformers\n",
      "\n",
      "Quantization is a technique used to reduce the precision of the model's weights and activations, which can significantly speed up inference and reduce memory usage. Instead of using 32-bit floating-point numbers (the standard in most deep learning models), quantization maps these values to a lower-precision representation, such as 16-bit or even 8-bit integers.\n",
      "\n",
      "#### How It Works:\n",
      "1. **Mapping Values**: The high-precision values (e.g., 32-bit floats) are mapped to a lower-precision range (e.g., 8-bit integers). This is often done using techniques like linear quantization or logarithmic quantization.\n",
      "2. **Post-Training Quantization**: This is typically applied after the model has been trained. The weights are quantized to a lower precision, and the model is fine-tuned if necessary to maintain accuracy.\n",
      "3. **Quantization-Aware Training**: In this approach, the model is trained with the knowledge that it will eventually be quantized. This can help maintain accuracy by accounting for the quantization process during training.\n",
      "\n",
      "#### Benefits:\n",
      "- **Reduced Memory Usage**: Lower-precision representations require less memory, making the model more suitable for deployment on edge devices.\n",
      "- **Faster Computation**: Lower-precision arithmetic (e.g., 8-bit integer operations) is faster than 32-bit floating-point operations, especially on hardware optimized for such computations.\n",
      "\n",
      "#### Trade-offs:\n",
      "- **Potential Accuracy Loss**: Quantization can lead to a loss in model accuracy, as the lower-precision representation may not capture the full range of information in the original weights.\n",
      "\n",
      "In summary, quantization is a powerful tool for accelerating transformers, but it must be applied carefully to avoid significant accuracy degradation.\n",
      "\n",
      "User: Who do you think is better Optimus prime or decepticons in terms of transformers\n",
      "Assistant: It seems there might be a bit of confusion here! The term \"Transformers\" in the context of machine learning refers to the Transformer architecture introduced in the paper \"Attention Is All You Need\" by Vaswani et al. (2017). This architecture is widely used in natural language processing and other sequence modeling tasks.\n",
      "\n",
      "On the other hand, \"Optimus Prime\" and \"Decepticons\" are fictional characters from the Transformers franchise, which is unrelated to the Transformer architecture in machine learning. If you're interested in learning more about the Transformer architecture itself, feel free to ask, and I'd be happy to explain it in detail!\n",
      "Thank you for your time and Goodbye :)\n"
     ]
    }
   ],
   "source": [
    "bot.Start_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the AI Chatbot!\n",
      "\n",
      "Hello! I am your AI-powered chatbot, here to assist you with your questions regarding transformers and Attention is all you need. \n",
      "The challenge here is to restrict questions only to the knowledge to Attention is all you need by Vaswani et all!\n",
      "\n",
      "Type \"exit\" or \"quit\" to end the conversation.\n",
      "\n",
      "Let's get started! Ask me why attention is all you need!?\n",
      "<class 'dict'>\n",
      "\n",
      "User: What is attention?\n",
      "Assistant: Attention in machine learning, particularly within Transformer models, is a mechanism that allows models to focus on relevant parts of input data dynamically. Here's a structured explanation:\n",
      "\n",
      "1. **Core Concept**: Attention enables models to weigh the importance of different input elements dynamically, allowing them to focus on relevant parts of the data.\n",
      "\n",
      "2. **Mechanism**:\n",
      "   - **Query, Key, Value Vectors**: Each element in the input is transformed into query, key, and value vectors. The query vector interacts with all key vectors to produce attention scores.\n",
      "   - **Attention Scores**: These scores determine how much each value vector contributes to the final output, allowing the model to focus on relevant parts of the input.\n",
      "\n",
      "3. **Self-Attention**: In self-attention, each word in a sentence queries all other words to see which are relevant. This helps the model understand relationships and context, especially useful in capturing long-range dependencies.\n",
      "\n",
      "4. **Multi-Head Attention**: This involves multiple attention mechanisms (heads) working in parallel, each focusing on different aspects of the input. Combining these results provides a richer representation, capturing various types of relationships.\n",
      "\n",
      "5. **Comparison with RNNs/LSTMs**: Unlike RNNs, which process sequences sequentially, attention allows the model to consider the entire sequence at once, potentially improving efficiency and effectiveness.\n",
      "\n",
      "6. **Computational Considerations**: Self-attention has a time complexity of O(n¬≤), which can be challenging for long sequences. However, optimizations like sparse attention can mitigate this.\n",
      "\n",
      "7. **Axial Attention**: This technique applies attention along different dimensions (e.g., rows and columns in an image), potentially reducing computational load.\n",
      "\n",
      "In summary, attention mechanisms are crucial in Transformer models, enabling dynamic weighting of input elements to enhance understanding and generation of contextually appropriate outputs.\n",
      "<class 'dict'>\n",
      "\n",
      "User: How is it different from RNNs\n",
      "Assistant: The key differences between attention mechanisms in Transformers and Recurrent Neural Networks (RNNs) can be summarized as follows:\n",
      "\n",
      "1. **Processing Methodology**:\n",
      "   - **RNNs**: Process input sequentially, one element at a time, maintaining a hidden state that carries information from previous steps.\n",
      "   - **Transformers**: Process all elements of the input in parallel, enabling faster computation and better handling of long sequences.\n",
      "\n",
      "2. **Handling Dependencies**:\n",
      "   - **RNNs**: Can struggle with long-range dependencies due to issues like vanishing or exploding gradients.\n",
      "   - **Transformers**: Utilize self-attention to capture long-range dependencies effectively, allowing each element to attend to every other element in the sequence.\n",
      "\n",
      "3. **Contextual Understanding**:\n",
      "   - **RNNs**: Limited by the hidden state's capacity to carry information, which can dilute as the sequence lengthens.\n",
      "   - **Transformers**: Employ multi-head attention, enabling simultaneous focus on multiple aspects of the input, providing a more comprehensive context.\n",
      "\n",
      "4. **Directionality**:\n",
      "   - **RNNs**: Typically unidirectional, with bidirectional variants processing each direction separately.\n",
      "   - **Transformers**: Inherently capture relationships in both directions within the same layer, enhancing efficiency.\n",
      "\n",
      "5. **Computational Efficiency**:\n",
      "   - **RNNs**: Harder to parallelize due to sequential dependencies.\n",
      "   - **Transformers**: Fully leverage parallel processing, making training faster and more efficient, especially on GPU resources.\n",
      "\n",
      "In summary, Transformers overcome many of RNNs' limitations by using attention mechanisms, offering superior performance in tasks requiring complex and long-range context understanding.\n",
      "<class 'dict'>\n",
      "\n",
      "User: How can we make transformers faster?\n",
      "Assistant: To make Transformers faster, several optimization techniques can be employed, each targeting different aspects of the model's architecture and computation:\n",
      "\n",
      "1. **Sparse Attention**: Reduces the number of attention operations by focusing only on relevant token interactions, thus lowering computational load.\n",
      "\n",
      "2. **Window-Based Attention**: Limits attention to local neighborhoods, as seen in models like the Local Transformer, which decreases the number of operations.\n",
      "\n",
      "3. **Low-Rank Approximations**: Decomposes the attention matrix into lower-rank components, enhancing computational efficiency.\n",
      "\n",
      "4. **Locality-Sensitive Hashing (LSH)**: Groups similar tokens, allowing the model to attend to a subset, improving efficiency.\n",
      "\n",
      "5. **Global Memory**: Enables attention to a fixed number of tokens, independent of input length, reducing computation time.\n",
      "\n",
      "6. **Relative Positional Encoding**: Enhances understanding of token relationships, aiding efficiency in certain contexts.\n",
      "\n",
      "7. **Efficient Implementation**: Utilizes optimized libraries and hardware (GPUs/TPUs) and techniques like gradient checkpointing to save memory and improve speed.\n",
      "\n",
      "These methods collectively aim to reduce computational complexity, making Transformers faster while maintaining performance. The choice of technique depends on the specific use case and desired balance between speed and accuracy.\n",
      "<class 'dict'>\n",
      "\n",
      "User: Can yo explain the 7th point in the first question again?\n",
      "Assistant: Certainly! The 7th point in the context of optimizing Transformers for speed involves efficient implementation strategies. Here's a breakdown of how each component contributes to making Transformers faster:\n",
      "\n",
      "1. **Optimized Libraries**: These libraries, such as TensorFlow, PyTorch, and cuDNN, are pre-optimized for performance. They provide efficient implementations of common operations used in Transformers, such as matrix multiplications and attention mechanisms, which helps speed up training and inference.\n",
      "\n",
      "2. **Hardware Acceleration**: Utilizing Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs) can significantly accelerate the computation. These specialized hardware units are designed to handle the parallel computations required by Transformers, making training and inference much faster compared to using Central Processing Units (CPUs).\n",
      "\n",
      "3. **Gradient Checkpointing**: This technique reduces memory usage during training by recomputing intermediate gradients during backpropagation instead of storing them. This allows for training larger models or using longer sequences without running into memory constraints, thereby improving efficiency.\n",
      "\n",
      "By leveraging these efficient implementation strategies, the training and inference processes of Transformers become faster and more efficient. These optimizations are crucial for practical applications, enabling the handling of larger inputs and quicker training times, which is essential for real-world deployment.\n",
      "<class 'dict'>\n",
      "\n",
      "User: Okay thanks but, I wanted to ask what is the average attention span of an average teenage in india.\n",
      "Assistant: My knowledge is strictly limited to attention mechanisms and Transformer models.\n",
      "Thank you for your time and Goodbye :)\n"
     ]
    }
   ],
   "source": [
    "bot.Start_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'prompt': span_df['attributes.llm.input_messages'],\n",
    "'response': span_df['attributes.llm.output_messages'],\n",
    "'timestamp': span_df['start_time'].iloc[-1], span_df['end_time'].iloc[-1],\n",
    "'latency': (span_df['end_time'].iloc[-1] - span_df['start_time'].iloc[-1]).total_seconds(),\n",
    "'model_name': span_df['attributes.llm.invocation_parameters']['model'],\n",
    "'trace_id': span_df['context.trace_id']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>span_kind</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>status_code</th>\n",
       "      <th>status_message</th>\n",
       "      <th>events</th>\n",
       "      <th>context.span_id</th>\n",
       "      <th>context.trace_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [name, span_kind, parent_id, start_time, end_time, status_code, status_message, events, context.span_id, context.trace_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import phoenix as px\n",
    "\n",
    "px.launch_app()\n",
    "\n",
    "span_df = px.Client().get_spans_dataframe(project_name=\"chatbot\")\n",
    "span_df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_df = span_df.iloc[-1]\n",
    "\n",
    "data = {\n",
    "    'prompt': span_df['attributes.llm.input_messages'],\n",
    "    'response': span_df['attributes.llm.output_messages'],\n",
    "    'start_time': str(span_df['start_time']),\n",
    "    'latency': str((span_df['end_time'] - span_df['start_time']).total_seconds()),\n",
    "    'model_name': span_df['attributes.llm.invocation_parameters']['model'],\n",
    "    'trace_id': span_df['context.trace_id']\n",
    "}\n",
    "time_stamp = date_time_stamp()\n",
    "with open(f\"tracing_{time_stamp}.json\", \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load traces and evaluate usig LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tracing_0330020905.json\", \"r\") as json_file:\n",
    "    tracing_data = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# CONTEXT: #\\nThis chat takes place between a curious user and an artificial intelligence assistant. The assistant\\'s knowledge is strictly limited to the content of the paper Attention Is All You Need by Vaswani et al. (2017).\\n\\n#########\\n\\n# OBJECTIVES #:\\nThe assistant must provide accurate, detailed, and precise answers based solely on the Attention Is All You Need paper. It must not reference, infer, or provide information beyond this paper. If a question falls outside its scope, the assistant should clearly state its limitation and refrain from speculation.\\n\\n#########\\n\\n# Style #:\\nThe responses should be contextually relevant, concise, and well-structured, providing only necessary details to answer the user‚Äôs queries effectively.\\n\\n#########\\n\\n# TONE #:\\nThe tone should be professional, informative, and neutral. The assistant should remain factual and avoid any assumptions or subjective opinions.\\n\\n#########\\n\\n# AUDIENCE #:\\nThis interaction is intended for users interested in understanding concepts strictly from Attention Is All You Need, such as researchers, students, and AI enthusiasts exploring the Transformer model.\\n\\n#########\\n\\n# RESPONSES #:\\n\\nThe assistant\\'s responses should be precise and aligned strictly with the paper.If the user\\'s question extends beyond the paper\\'s content, the assistant should reply: ‚ÄúMy knowledge is strictly limited to the paper Attention Is All You Need, and I cannot provide an answer beyond its scope.‚Äù. The conversation should maintain clarity and relevance to the paper\\'s key topics, including self-attention, multi-head attention, and Transformer architecture.\\nUser: What is attention?\\nAssistant: Based on the provided context from the paper \"Attention Is All You Need\" by Vaswani et al. (2017), attention refers to the process of measuring various forms of language understanding by requiring a certain type of reasoning over the linguistic facts presented in each story. In this context, attention is used to identify the relevant facts and questions, and to adjust the processing steps accordingly.\\nUSER: \\nContext: A detailed performance comparison is given in thecomputation efÔ¨Åciency section.Loss function We choose the MSE loss function on pre-diction w.r.t the target sequences, and the loss is propagatedback from the decoder‚Äôs outputs across the entire model. 4ExperimentDatasetsWe empirically perform experiments on four datasets, in-cluding 2 collected real-world datasets for LSTF and 2 pub-lic benchmark datasets.ETT (Electricity Transformer Temperature)2: The ETT isa crucial indicator in the electric power long-term deploy-ment. We collected 2 years data from two separated countiesin China. To explorer the granularity on the LSTF problem,we create separate datasets as {ETTh1, ETTh2} for 1-hour-level and ETTm1 for 15-minutes-level. Each data point con-sists of the target value ‚Äùoil temperature‚Äù and 6 power loadfeatures.\\n\\nThe train/val/test is 12/4/4 months.ECL (Electricity Consuming Load)3: It collects the elec-tricity consumption (Kwh) of 321 clients. Due to the missingdata (Li et al. 2019), we convert the dataset into hourly con-sumption of 2 years and set ‚ÄòMT 320‚Äô as the target value.The train/val/test is 15/3/4 months.Weather 4: This dataset contains local climatological datafor nearly 1,600 U.S. locations, 4 years from 2010 to 2013,where data points are collected every 1 hour. Each data pointconsists of the target value ‚Äùwet bulb‚Äù and 11 climate fea-tures. The train/val/test is 28/10/10 months.Experimental DetailsWe brieÔ¨Çy summarize basics, and more information on net-work components and setups are given in Appendix E.Baselines: The details of network components are givenin Appendix E.1. We have selected 5 time-series fore-casting methods as comparison, including ARIMA (Ariyo,Adewumi, and Ayo 2014), Prophet (Taylor and Letham2018), LSTMa (Bahdanau, Cho, and Bengio 2015) andLSTnet (Lai et al.\\n\\n2018) and DeepAR (Flunkert, Salinas,and Gasthaus 2017). To better explore the ProbSparse self-attention‚Äôs performance in our proposed Informer, we in-corporate the canonical self-attention variant (Informer‚Ä†),the efÔ¨Åcient variant Reformer (Kitaev, Kaiser, and Levskaya\\n2We collected the ETT dataset and published it at https://github.com/zhouhaoyi/ETDataset.3ECL dataset was acquired at https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams201120144Weather dataset was acquired at https://www.ncdc.noaa.gov/orders/qclcd/\\n2019) and the most related work LogSparse self-attention(Li et al. 2019) in the experiments.Hyper-parameter tuning: We conduct grid search overthe hyper-parameters and detail ranges are given in Ap-pendix E.3. Informer contains a 3-layer stack and a 2-layerstack (1/4 input) in encoder, 2-layer decoder.\\nASSISTANT: \\nBased on the provided context from the paper \"Attention Is All You Need\" by Vaswani et al. (2017), the experiment section discusses the performance comparison of the proposed Informer model with other time-series forecasting methods on four datasets: ETT, ECL, Weather, and public benchmark datasets.\\n\\nThe paper presents a detailed comparison of the Informer model\\'s performance with five baselines: ARIMA, Prophet, LSTMa, LSTnet, and DeepAR. The experiment also includes a comparison with the canonical self-attention variant, the efficient variant Reformer, and the most related work LogSparse self-attention.\\n\\nThe hyper-parameters of the Informer model are tuned using a grid search, and the detailed ranges are provided in Appendix E.3. The model architecture consists of a 3-layer stack and a 2-layer stack (1/4 input) in the encoder, and a 2-layer decoder.\\n\\nPlease note that my knowledge is strictly limited to the paper \"Attention Is All You Need\" by Vaswani et al. (2017), and I cannot provide information beyond its scope.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tracing_data['prompt'][0]['message.content'] + '\\n' + tracing_data['response'][0]['message.content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = Groq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are an AI assistant tasked with evaluating the quality of a chatbot's response. \n",
    "Given the following conversation between a user and the chatbot, assess the chatbot's response based on the following criteria:\n",
    "\n",
    "1. Relevance: Is the response relevant to the user's query?\n",
    "2. Clarity: Is the response clear and easy to understand?\n",
    "3. Completeness: Does the response provide sufficient information to address the user's query?\n",
    "4. Accuracy: Is the information provided in the response factually correct?\n",
    "\n",
    "Conversation:\n",
    "{conversation}\n",
    "\n",
    "Please provide a **detailed evaluation** for each criterion and assign an **overall score (out of 10)** for the chatbot's response. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = LLM.chat.completions.create(\n",
    "    messages = prompt.format(data=data), \n",
    "    model = 'deepseek-r1-distill-qwen-32b',\n",
    "    temperature = 0.2,\n",
    ").choices[0].message.content\n",
    "response = remove_think_section(response)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aditya-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
